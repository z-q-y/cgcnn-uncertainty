{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05/06/2019 To-do list:\n",
    "1. Calibration curve analog for regression (edited) \n",
    "2. Indicate how many points are below/above each parity line\n",
    "3. Plot multiple cases side by side on the same graph (e.g. test set with 50 pts & 1500 pts)\n",
    "4. Manipulating dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06/14/2019 To-do list:\n",
    "1. If process gets stuck on SDT transform (tqdm) step:\n",
    "   a. Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document demonstrates the making, training, saving, loading, and usage of a sklearn-compliant CGCNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pylon5/ch5fq5p/zulissi/miniconda3/envs/cgcnn/lib/python3.6/site-packages/IPython/config.py:13: ShimWarning: The `IPython.config` package has been deprecated since IPython 4.0. You should import from traitlets.config instead.\n",
      "  \"You should import from traitlets.config instead.\", ShimWarning)\n",
      "/pylon5/ch5fq5p/zulissi/miniconda3/envs/cgcnn/lib/python3.6/site-packages/ipycache.py:17: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
      "  from IPython.utils.traitlets import Unicode\n"
     ]
    }
   ],
   "source": [
    "%load_ext ipycache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "#Comment/add these\n",
    "sys.path.insert(0,'../')\n",
    "sys.path.insert(0,'/home/zulissi/software/adamwr/')\n",
    "\n",
    "import numpy as np\n",
    "import cgcnn\n",
    "\n",
    "import time\n",
    "\n",
    "#Select which GPU to use if necessary\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset as mongo docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This operation took 14.900000000000002 s.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pickle\n",
    "\n",
    "starttime = time.clock()\n",
    "#Load a selection of documents\n",
    "docs = pickle.load(open('/pylon5/ch5fq5p/zulissi/CO_docs.pkl','rb'))\n",
    "random.seed(42)\n",
    "random.shuffle(docs)\n",
    "docs = [doc for doc in docs if -3<doc['energy']<1.0]\n",
    "docs = docs[:6000]\n",
    "\n",
    "endtime = time.clock()\n",
    "print('This operation took', endtime - starttime, 's.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the size of the features from the data transformer, to be used in setting up the net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1201/6000 [07:26<24:25,  3.27it/s]  "
     ]
    }
   ],
   "source": [
    "# %%cache SDT_list.pkl SDT_list\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import mongo\n",
    "from cgcnn.data import StructureData, ListDataset, StructureDataTransformer\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "SDT = StructureDataTransformer(atom_init_loc='../atom_init.json',\n",
    "                              max_num_nbr=12,\n",
    "                               step=0.2,\n",
    "                              radius=1,\n",
    "                              use_tag=False,\n",
    "                              use_fixed_info=False,\n",
    "                              use_distance=True)\n",
    "\n",
    "SDT_out = SDT.transform(docs)\n",
    "\n",
    "structures = SDT_out[0]\n",
    "\n",
    "# Settings necessary to build the model (since they are size of vectors as inputs)\n",
    "orig_atom_fea_len = structures[0].shape[-1]\n",
    "nbr_fea_len = structures[1].shape[-1]\n",
    "\n",
    "import multiprocess as mp\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "SDT_out = SDT.transform(docs)\n",
    "\n",
    "with mp.Pool(4) as pool:\n",
    "    SDT_list = list(tqdm.tqdm(pool.imap(lambda x: SDT_out[x],range(len(SDT_out)),chunksize=40),total=len(SDT_out)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = time.clock()\n",
    "with open('distance_all_docs.pkl','wb') as fhandle:\n",
    "    pickle.dump(SDT_list,fhandle)\n",
    "\n",
    "endtime = time.clock()\n",
    "print('This step took', endtime - starttime, 's to complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CGCNN model with skorch to make it sklearn compliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam, SGD\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from skorch.callbacks import Checkpoint, LoadInitState #needs skorch 0.4.0, conda-forge version at 0.3.0 doesn't cut it\n",
    "from cgcnn.data import collate_pool\n",
    "from skorch import NeuralNetRegressor\n",
    "from cgcnn.model_no_dropout import CrystalGraphConvNet\n",
    "import torch\n",
    "from cgcnn.data import MergeDataset\n",
    "import skorch.callbacks.base\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device='cpu'\n",
    "\n",
    "startime = time.clock()\n",
    "\n",
    "#Make a checkpoint to save parameters every time there is a new best for validation lost\n",
    "cp = Checkpoint(monitor='valid_loss_best',fn_prefix='valid_best_')\n",
    "\n",
    "#Callback to load the checkpoint with the best validation loss at the end of training\n",
    "class train_end_load_best_valid_loss(skorch.callbacks.base.Callback):\n",
    "    def on_train_end(self, net, X, y):\n",
    "        net.load_params('valid_best_params.pt')\n",
    "        \n",
    "load_best_valid_loss = train_end_load_best_valid_loss()\n",
    "\n",
    "endtime = time.clock()\n",
    "print('This step takes', endtime - startime, 's to complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\color{red}{This seems to be a time consuming step.}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example converting all the documents up front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = time.clock()\n",
    "#Make the target list\n",
    "target_list = np.array([doc['energy'] for doc in docs]).reshape(-1,1)\n",
    "\n",
    "endtime = time.clock()\n",
    "print('This step takes', endtime - startime, 's to complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "starttime = time.clock()\n",
    "SDT_training, SDT_test, target_training, target_test = train_test_split(SDT_list, target_list, test_size=0.2)\n",
    "\n",
    "endtime = time.clock()\n",
    "print('This step takes', endtime - startime, 's to complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.dataset import CVSplit\n",
    "from skorch.callbacks.lr_scheduler import WarmRestartLR, LRScheduler\n",
    "\n",
    "train_test_splitter = ShuffleSplit(test_size=0.25) # , random_state=42)\n",
    "LR_schedule = LRScheduler('MultiStepLR',milestones=[100],gamma=0.1)\n",
    "\n",
    "net = NeuralNetRegressor(\n",
    "    CrystalGraphConvNet,\n",
    "    module__orig_atom_fea_len = orig_atom_fea_len,\n",
    "    module__nbr_fea_len = nbr_fea_len,\n",
    "    # module__dropout = 0.2,\n",
    "    batch_size=214,\n",
    "    module__classification=False,\n",
    "    lr=0.0056,\n",
    "    max_epochs=188, # 292\n",
    "    module__atom_fea_len=46,\n",
    "    module__h_fea_len=83,\n",
    "    module__n_conv=8,\n",
    "    module__n_h=4,\n",
    "    optimizer=Adam,\n",
    "    iterator_train__pin_memory=True,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__collate_fn = collate_pool,\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_valid__pin_memory=True,\n",
    "    iterator_valid__num_workers=0,\n",
    "    iterator_valid__collate_fn = collate_pool,\n",
    "    device=device,\n",
    "    criterion=torch.nn.MSELoss,\n",
    "#   criterion=torch.nn.L1Loss,\n",
    "    dataset=MergeDataset,\n",
    "    train_split = CVSplit(cv=train_test_splitter),\n",
    "    callbacks=[cp, load_best_valid_loss, LR_schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Change This!!!\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def plot(SDT_training, SDT_test, target_training, target_test, train_indices, valid_indices, net):\n",
    "    # train_indices, valid_indices = next(train_test_splitter.split(SDT_training))\n",
    "    # lentotal = len(SDT_training)\n",
    "    # train_indices = list(range(round(3/4 * lentotal)))\n",
    "    # valid_indices = list(range(round(3/4 * lentotal), lentotal))\n",
    "\n",
    "    training_data = {'actual_value':np.array(target_training.reshape(-1))[train_indices],\n",
    "                     'predicted_value':net.predict(SDT_training)[train_indices].reshape(-1)}\n",
    "    test_data = {'actual_value':np.array(target_test).reshape(-1),\n",
    "                 'predicted_value':net.predict(SDT_test).reshape(-1)}\n",
    "    validation_data = {'actual_value':np.array(target_training.reshape(-1))[valid_indices],\n",
    "                       'predicted_value':net.predict(SDT_training)[valid_indices].reshape(-1)}\n",
    "\n",
    "    df_training = pd.DataFrame(training_data)\n",
    "    df_validation = pd.DataFrame(validation_data)\n",
    "    df_test = pd.DataFrame(test_data)\n",
    "    \n",
    "    f, ax = plt.subplots(figsize=(8,8))\n",
    "    ax.scatter(df_training['actual_value'], df_training['predicted_value'], color='orange', \n",
    "               marker='o', alpha=0.5, label='train\\nMAE=%0.2f, RMSE=%0.2f, R$^2$=%0.2f'\\\n",
    "                %(mean_absolute_error(df_training['actual_value'], df_training['predicted_value']), \n",
    "                  np.sqrt(mean_squared_error(df_training['actual_value'], df_training['predicted_value'])),\n",
    "                  r2_score(df_training['actual_value'], df_training['predicted_value'])))\n",
    "\n",
    "    ax.scatter(df_validation['actual_value'], df_validation['predicted_value'], color='blue', \n",
    "               marker='o', alpha=0.5, label='valid\\nMAE=%0.2f, RMSE=%0.2f, R$^2$=%0.2f'\\\n",
    "                %(mean_absolute_error(df_validation['actual_value'], df_validation['predicted_value']), \n",
    "                  np.sqrt(mean_squared_error(df_validation['actual_value'], df_validation['predicted_value'])),\n",
    "                  r2_score(df_validation['actual_value'], df_validation['predicted_value'])))\n",
    "\n",
    "    ax.scatter(df_test['actual_value'], df_test['predicted_value'], color='green', \n",
    "               marker='o', alpha=0.5, label='test\\nMAE=%0.2f, RMSE=%0.2f, R$^2$=%0.2f'\\\n",
    "                %(mean_absolute_error(df_test['actual_value'], df_test['predicted_value']), \n",
    "                  np.sqrt(mean_squared_error(df_test['actual_value'], df_test['predicted_value'])),\n",
    "                  r2_score(df_test['actual_value'], df_test['predicted_value'])))\n",
    "\n",
    "\n",
    "    ax.plot([min(df_training['actual_value']), max(df_training['actual_value'])], \n",
    "            [min(df_training['actual_value']), max(df_training['actual_value'])], 'k--')\n",
    "\n",
    "    # format graph\n",
    "    ax.tick_params(labelsize=14)\n",
    "    ax.set_xlabel('DFT E (eV)', fontsize=14)\n",
    "    ax.set_ylabel('CGCNN predicted E (eV)', fontsize=14)\n",
    "    ax.set_title('Multi-element ', fontsize=14) \n",
    "    ax.legend(fontsize=12)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return df_training, df_validation, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TrainingData = []\n",
    "ValidationData = []\n",
    "TestData = []\n",
    "\n",
    "iters = 7\n",
    "\n",
    "starttime = time.clock()\n",
    "\n",
    "train_test_splitter = ShuffleSplit(test_size=0.25, random_state=42)\n",
    "train_indices, valid_indices = next(train_test_splitter.split(SDT_training))\n",
    "\n",
    "net.initialize()\n",
    "net.fit(SDT_training, target_training)\n",
    "\n",
    "for i in range(iters):\n",
    "    # net()\n",
    "    # net.initialize()\n",
    "    train_test_splitter = ShuffleSplit(test_size=0.25, random_state=42)\n",
    "    train_indices, valid_indices = next(train_test_splitter.split(SDT_training))\n",
    "    \n",
    "    print(\"train_indices:\", train_indices)\n",
    "    print(\"valid_indices:\", valid_indices)\n",
    "    \n",
    "    with open('no-dropout_log.txt', 'a') as logfile:\n",
    "        logfile.write(\"Iter: %s\" % (i,))\n",
    "        logfile.write(\"train_indices: %s\" % (train_indices,))\n",
    "        logfile.write(\"train_indices: %s\\n\" % (train_indices,))\n",
    "\n",
    "    # net.fit(SDT_training, target_training)    \n",
    "    dftraining, dfvalidation, dftest = plot(SDT_training,\n",
    "                                            SDT_test,\n",
    "                                            target_training,\n",
    "                                            target_test,\n",
    "                                            train_indices,\n",
    "                                            valid_indices, net)\n",
    "    \n",
    "    TrainingData.append(dftraining)\n",
    "    ValidationData.append(dfvalidation)\n",
    "    TestData.append(dftest)\n",
    "    \n",
    "TrainingData = pd.concat(TrainingData, axis=1)\n",
    "ValidationData = pd.concat(ValidationData, axis=1)\n",
    "TestData = pd.concat(TestData, axis=1)\n",
    "\n",
    "endtime = time.clock()\n",
    "print(\"Calculating the same points {} times takes {} s.\".format(iters, endtime-starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The d20 suffix means a droupout of 20% is applied\n",
    "TrainingData.to_pickle('TrData_7iters_d20.pkl')\n",
    "ValidationData.to_pickle('VlData_7iters_d20.pkl')\n",
    "TestData.to_pickle('TsData_7iters_d20.pkl')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# The d30 suffix means a droupout of 30% is applied\n",
    "TrainingData.to_pickle('TrData_7iters_d30.pkl')\n",
    "ValidationData.to_pickle('VlData_7iters_d30.pkl')\n",
    "TestData.to_pickle('TsData_7iters_d30.pkl')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingData.to_pickle('TrData_7iters_vanilla_081319.pkl')\n",
    "ValidationData.to_pickle('VlData_7iters_vanilla_081319.pkl')\n",
    "TestData.to_pickle('TsData_7iters_vanilla_081319.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cgcnn_conda",
   "language": "python",
   "name": "cgcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
