{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document demonstrates the making, training, saving, loading, and usage of a sklearn-compliant CGCNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cgcnn\n",
    "\n",
    "#Select which GPU to use if necessary\n",
    "#%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "#%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset as mongo docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "\n",
    "#Load a selection of documents\n",
    "docs = pickle.load(open('CO_docs.pkl','rb'))\n",
    "random.shuffle(docs)\n",
    "docs = [doc for doc in docs if -3<doc['energy']<1.0]\n",
    "docs = docs[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the size of the features from the data transformer, to be used in setting up the net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import mongo\n",
    "from cgcnn.data import StructureData, ListDataset, StructureDataTransformer\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "SDT = StructureDataTransformer(atom_init_loc='/home/zulissi/software/cgcnn_sklearn/atom_init.json',\n",
    "                              max_num_nbr=12,\n",
    "                               step=0.2,\n",
    "                              radius=1,\n",
    "                              use_tag=True,\n",
    "                              use_fixed_info=True)\n",
    "\n",
    "SDT_out = SDT.transform(docs)\n",
    "\n",
    "structures = SDT_out[0]\n",
    "\n",
    "#Settings necessary to build the model (since they are size of vectors as inputs)\n",
    "orig_atom_fea_len = structures[0].shape[-1]\n",
    "nbr_fea_len = structures[1].shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CGCNN model with skorch to make it sklearn compliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam, SGD\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from skorch.callbacks import Checkpoint, LoadInitState #needs skorch 0.4.0, conda-forge version at 0.3.0 doesn't cut it\n",
    "from cgcnn.data import collate_pool\n",
    "from skorch import NeuralNetRegressor\n",
    "from cgcnn.model import CrystalGraphConvNet\n",
    "import torch\n",
    "from cgcnn.data import MergeDataset\n",
    "import skorch.callbacks.base\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device='cpu'\n",
    "\n",
    "#Make a checkpoint to save parameters every time there is a new best for validation lost\n",
    "cp = Checkpoint(monitor='valid_loss_best',fn_prefix='valid_best_')\n",
    "\n",
    "#Callback to load the checkpoint with the best validation loss at the end of training\n",
    "class train_end_load_best_valid_loss(skorch.callbacks.base.Callback):\n",
    "    def on_train_end(self, net, X, y):\n",
    "        net.load_params('valid_best_params.pt')\n",
    "        \n",
    "load_best_valid_loss = train_end_load_best_valid_loss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example converting all the documents up front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import multiprocess as mp\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "SDT_out = SDT.transform(docs)\n",
    "\n",
    "with mp.Pool(4) as pool:\n",
    "    SDT_list = list(tqdm.tqdm(pool.imap(lambda x: SDT_out[x],range(len(SDT_out)),chunksize=40),total=len(SDT_out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the target list\n",
    "target_list = np.array([doc['energy'] for doc in docs]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SDT_training, SDT_test, target_training, target_test = train_test_split(SDT_list, target_list, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.dataset import CVSplit\n",
    "from skorch.callbacks.lr_scheduler import WarmRestartLR, LRScheduler\n",
    "\n",
    "train_test_splitter = ShuffleSplit(test_size=0.25, random_state=42)\n",
    "LR_schedule = LRScheduler('MultiStepLR',milestones=[100],gamma=0.1)\n",
    "\n",
    "net = NeuralNetRegressor(\n",
    "    CrystalGraphConvNet,\n",
    "    module__orig_atom_fea_len = orig_atom_fea_len,\n",
    "    module__nbr_fea_len = nbr_fea_len,\n",
    "    batch_size=214,\n",
    "    module__classification=False,\n",
    "    lr=0.0056,\n",
    "    max_epochs=292, #188\n",
    "    module__atom_fea_len=46,\n",
    "    module__h_fea_len=83,\n",
    "    module__n_conv=8,\n",
    "    module__n_h=4,\n",
    "    optimizer=Adam,\n",
    "    iterator_train__pin_memory=True,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__collate_fn = collate_pool,\n",
    "    iterator_valid__pin_memory=True,\n",
    "    iterator_valid__num_workers=0,\n",
    "    iterator_valid__collate_fn = collate_pool,\n",
    "    device=device,\n",
    "    criterion=torch.nn.MSELoss,\n",
    "    dataset=MergeDataset,\n",
    "    train_split = CVSplit(cv=train_test_splitter),\n",
    "    callbacks=[cp, load_best_valid_loss, LR_schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize()\n",
    "net.fit(SDT_training,target_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_indices, valid_indices = next(train_test_splitter.split(SDT_training))\n",
    "\n",
    "training_data = {'actual_value':np.array(target_training.reshape(-1))[train_indices],\n",
    "                 'predicted_value':net.predict(SDT_training)[train_indices].reshape(-1)}\n",
    "test_data ={'actual_value':np.array(target_test).reshape(-1),\n",
    "            'predicted_value':net.predict(SDT_test).reshape(-1)}\n",
    "validation_data = {'actual_value':np.array(target_training.reshape(-1))[valid_indices],\n",
    "                 'predicted_value':net.predict(SDT_training)[valid_indices].reshape(-1)}\n",
    "\n",
    "df_training = pd.DataFrame(training_data)\n",
    "df_validation = pd.DataFrame(validation_data)\n",
    "df_test = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.to_csv('training.csv', sep='\\t', index=False)\n",
    "df_test.to_csv('test.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "ax.scatter(df_training['actual_value'], df_training['predicted_value'], color='orange', \n",
    "           marker='o', alpha=0.5, label='train\\nMAE=%0.2f, RMSE=%0.2f, R$^2$=%0.2f'\\\n",
    "            %(mean_absolute_error(df_training['actual_value'], df_training['predicted_value']), \n",
    "              np.sqrt(mean_squared_error(df_training['actual_value'], df_training['predicted_value'])),\n",
    "              r2_score(df_training['actual_value'], df_training['predicted_value'])))\n",
    "\n",
    "\n",
    "ax.scatter(df_validation['actual_value'], df_validation['predicted_value'], color='blue', \n",
    "           marker='o', alpha=0.5, label='valid\\nMAE=%0.2f, RMSE=%0.2f, R$^2$=%0.2f'\\\n",
    "            %(mean_absolute_error(df_validation['actual_value'], df_validation['predicted_value']), \n",
    "              np.sqrt(mean_squared_error(df_validation['actual_value'], df_validation['predicted_value'])),\n",
    "              r2_score(df_validation['actual_value'], df_validation['predicted_value'])))\n",
    "\n",
    "ax.scatter(df_test['actual_value'], df_test['predicted_value'], color='green', \n",
    "           marker='o', alpha=0.5, label='test\\nMAE=%0.2f, RMSE=%0.2f, R$^2$=%0.2f'\\\n",
    "            %(mean_absolute_error(df_test['actual_value'], df_test['predicted_value']), \n",
    "              np.sqrt(mean_squared_error(df_test['actual_value'], df_test['predicted_value'])),\n",
    "              r2_score(df_test['actual_value'], df_test['predicted_value'])))\n",
    "\n",
    "\n",
    "ax.plot([min(df_training['actual_value']), max(df_training['actual_value'])], \n",
    "        [min(df_training['actual_value']), max(df_training['actual_value'])], 'k--')\n",
    "\n",
    "# format graph\n",
    "ax.tick_params(labelsize=14)\n",
    "ax.set_xlabel('DFT E (eV)', fontsize=14)\n",
    "ax.set_ylabel('CGCNN predicted E (eV)', fontsize=14)\n",
    "ax.set_title('Multi-element ', fontsize=14) \n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
