{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dataset of docs taken from jupyter-dev using:\n",
    "#     from gaspy import gasdb, defaults\n",
    "#     import warnings\n",
    "#     warnings.filterwarnings('ignore')\n",
    "\n",
    "#     filters = defaults.adsorption_filters('CO')\n",
    "#     #filters['results.energy'] = {'$gt': -3.5, '$lt': 9.0}\n",
    "#     #filters['processed_data.movement_data.max_adsorbate_movement']['$lt'] = 4.0 #specifically for OOH vs OH\n",
    "\n",
    "#     # Establish the fingerprints that are needed for the preprocessing\n",
    "#     fingerprints = {}\n",
    "#     fingerprints['atoms']='$atoms'\n",
    "#     fingerprints['results']='$results'\n",
    "#     fingerprints['max_surface_movement']='$processed_data.movement_data.max_surface_movement'\n",
    "#     fingerprints['adsorption_site'] = '$initial_configuration.atoms.atoms'\n",
    "#     # Pull the documents and then modify them so that they'll work with the preprocessor\n",
    "#     docs = gasdb.get_adsorption_docs(['CO'],extra_fingerprints=fingerprints, filters=filters)\n",
    "\n",
    "#CO_docs = pickle.load(open('/home/zulissi/CO_docs_200.pkl','rb'))\n",
    "    \n",
    "\n",
    "# with open('/home/zulissi/CO_docs_200.pkl','wb') as fhandle:\n",
    "#     pickle.dump(CO_docs,fhandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset as mongo docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "\n",
    "docs = pickle.load(open('/home/zulissi/CO_docs.pkl','rb'))\n",
    "random.shuffle(docs)\n",
    "docs = docs[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the size of the features from the data transformer, to be used in setting up the net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import mongo\n",
    "from cgcnn.data import StructureData, ListDataset, StructureDataTransformer\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "energies = np.array([doc['energy'] for doc in docs])\n",
    "scaler = StandardScaler().fit(energies.reshape(-1, 1))\n",
    "\n",
    "\n",
    "SDT = StructureDataTransformer(atom_init_loc='/home/zulissi/software/cgcnn_sklearn/atom_init.json',\n",
    "                              max_num_nbr=9,\n",
    "                              radius=1,\n",
    "                              use_tag=True,\n",
    "                              use_fixed_info=True)\n",
    "\n",
    "SDT_out = SDT.transform(docs)\n",
    "\n",
    "structures = SDT_out[0]\n",
    "orig_atom_fea_len = structures[0].shape[-1]\n",
    "nbr_fea_len = structures[1].shape[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CGCNN model with skorch to make it sklearn compliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from skorch.callbacks import Checkpoint, LoadInitState #needs skorch 0.4.0, conda-forge version at 0.3.0 doesn't cut it\n",
    "from cgcnn.data import collate_pool\n",
    "from skorch import NeuralNetRegressor\n",
    "from cgcnn.model import CrystalGraphConvNet\n",
    "import torch\n",
    "from cgcnn.data import MergeDataset\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device='cpu'\n",
    "\n",
    "#This is a little weird, one of the parameters basically needs to match the results \n",
    "# that are coming from the transform function, should be fixable \n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_test_split(X,y):\n",
    "    kf = KFold(n_splits=5)\n",
    "    train_idx, test_idx = next(kf.split(X))\n",
    "    \n",
    "    dataset_train = list(zip(X[train_idx],y[train_idx]))\n",
    "    dataset_test = list(zip(X[test_idx],y[test_idx]))\n",
    "    \n",
    "    sjk5352\n",
    "    return dataset_train, dataset_test\n",
    "\n",
    " \n",
    "cp = Checkpoint(monitor='valid_loss_best')\n",
    "net = NeuralNetRegressor(\n",
    "    CrystalGraphConvNet,\n",
    "    module__orig_atom_fea_len = orig_atom_fea_len,\n",
    "    module__nbr_fea_len = nbr_fea_len,\n",
    "    module__atom_fea_len=46,\n",
    "    module__h_fea_len=83,\n",
    "    module__n_conv=8,\n",
    "    module__n_h=4,\n",
    "    iterator_train__batch_size=214,\n",
    "    iterator_train__pin_memory=True,\n",
    "    #iterator_train__num_workers=0,\n",
    "    iterator_train__collate_fn = collate_pool,\n",
    "    iterator_valid__pin_memory=True,\n",
    "    #iterator_valid__num_workers=0,\n",
    "    iterator_valid__collate_fn = collate_pool,\n",
    "    max_epochs=10,\n",
    "    lr=np.exp(-5.18),\n",
    "    optimizer=Adam,    \n",
    "    device=device,\n",
    "    criterion=torch.nn.L1Loss,\n",
    "    dataset=MergeDataset,\n",
    "    callbacks=[cp]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example converting all the documents up front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:25<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.7769\u001b[0m        \u001b[32m2.8486\u001b[0m     +  0.7164\n",
      "      2        1.5297        \u001b[32m0.8276\u001b[0m     +  0.2098\n",
      "      3        0.8223        \u001b[32m0.8070\u001b[0m     +  0.1989\n",
      "      4        0.8359        0.9374        0.1956\n",
      "      5        0.9581        0.9444        0.1960\n",
      "      6        0.9710        0.8743        0.1957\n",
      "      7        0.9193        0.8117        0.1953\n",
      "      8        0.8518        \u001b[32m0.7725\u001b[0m     +  0.1955\n",
      "      9        0.7934        \u001b[32m0.7532\u001b[0m     +  0.1954\n",
      "     10        \u001b[36m0.7644\u001b[0m        \u001b[32m0.7438\u001b[0m     +  0.1955\n"
     ]
    }
   ],
   "source": [
    "import multiprocess as mp\n",
    "\n",
    "SDT_out = SDT.transform(docs)\n",
    "\n",
    "with mp.Pool(4) as pool:\n",
    "    SDT_list = list(tqdm.tqdm(pool.imap(lambda x: SDT_out[x],range(len(SDT_out)),chunksize=40),total=len(SDT_out)))\n",
    "\n",
    "#Make the target list\n",
    "target_list = scaler.transform(np.array([doc['energy'] for doc in docs]).reshape(-1,1))\n",
    "y = torch.FloatTensor(target_list)\n",
    "\n",
    "#Fit the NN\n",
    "net.fit(SDT_list,y=y)\n",
    "\n",
    "#Load the best parameters (best validation)\n",
    "net.load_params('params.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test saving and loading and using a pipeline (single-threaded conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(SDT,net)\n",
    "\n",
    "#Save the fitted sklearn-compatible pipeline\n",
    "with open('fitted-pipeline.pkl','wb') as fhandle:\n",
    "    pickle.dump(pipe,fhandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37539738],\n",
       "       [ 0.5736287 ],\n",
       "       [ 0.2945856 ],\n",
       "       [ 0.28321084],\n",
       "       [-0.60433006],\n",
       "       [ 0.7196084 ],\n",
       "       [ 0.5606486 ],\n",
       "       [-0.6748233 ],\n",
       "       [ 0.46784127],\n",
       "       [-1.0971079 ],\n",
       "       [ 0.2739984 ],\n",
       "       [-1.0812374 ],\n",
       "       [ 0.78952384],\n",
       "       [ 0.64816505],\n",
       "       [ 0.89845055],\n",
       "       [ 0.80677325],\n",
       "       [ 0.2926386 ],\n",
       "       [ 0.83383465],\n",
       "       [ 0.91047287],\n",
       "       [ 1.0416356 ],\n",
       "       [-1.0808791 ],\n",
       "       [-0.32229933],\n",
       "       [ 0.6984915 ],\n",
       "       [-0.32724988],\n",
       "       [ 1.0069227 ],\n",
       "       [-1.1325859 ],\n",
       "       [ 0.7002761 ],\n",
       "       [ 0.35284716],\n",
       "       [ 0.31941128],\n",
       "       [ 0.74202013],\n",
       "       [ 0.70782155],\n",
       "       [ 0.95855284],\n",
       "       [ 0.44919765],\n",
       "       [ 0.75663906],\n",
       "       [-1.1704398 ],\n",
       "       [-0.99034244],\n",
       "       [ 1.0443858 ],\n",
       "       [ 0.4120098 ],\n",
       "       [ 0.55018395],\n",
       "       [-1.220707  ],\n",
       "       [ 0.6030986 ],\n",
       "       [-0.3989029 ],\n",
       "       [ 0.9047002 ],\n",
       "       [-0.88523614],\n",
       "       [-0.38830686],\n",
       "       [-0.330577  ],\n",
       "       [ 0.15086561],\n",
       "       [ 0.8366004 ],\n",
       "       [ 0.13825338],\n",
       "       [ 0.78022236],\n",
       "       [-0.4963615 ],\n",
       "       [ 0.25469276],\n",
       "       [ 0.59367466],\n",
       "       [-1.0587549 ],\n",
       "       [ 0.7102751 ],\n",
       "       [-1.0910378 ],\n",
       "       [ 0.9981567 ],\n",
       "       [ 0.7239883 ],\n",
       "       [ 0.11277723],\n",
       "       [ 0.27163878],\n",
       "       [ 0.54122466],\n",
       "       [-1.1391422 ],\n",
       "       [-0.8796573 ],\n",
       "       [ 0.2897965 ],\n",
       "       [ 0.83573747],\n",
       "       [ 0.26070687],\n",
       "       [-1.1577607 ],\n",
       "       [ 0.11371642],\n",
       "       [-0.99298733],\n",
       "       [-0.4927978 ],\n",
       "       [ 0.3432872 ],\n",
       "       [ 0.5662834 ],\n",
       "       [-0.5992961 ],\n",
       "       [ 0.82011056],\n",
       "       [-0.32405487],\n",
       "       [-0.43706387],\n",
       "       [-0.28008434],\n",
       "       [-0.59407693],\n",
       "       [ 0.06123427],\n",
       "       [ 0.84627867],\n",
       "       [ 0.6535424 ],\n",
       "       [-0.81953627],\n",
       "       [ 0.41029862],\n",
       "       [ 0.4898434 ],\n",
       "       [ 0.23786868],\n",
       "       [-0.9564491 ],\n",
       "       [ 0.37626302],\n",
       "       [ 0.21840633],\n",
       "       [ 0.33767596],\n",
       "       [ 0.8313828 ],\n",
       "       [ 0.3975193 ],\n",
       "       [-0.7016398 ],\n",
       "       [ 0.8363653 ],\n",
       "       [-1.0855516 ],\n",
       "       [-0.16328476],\n",
       "       [ 0.7024783 ],\n",
       "       [ 0.5576088 ],\n",
       "       [ 0.7837023 ],\n",
       "       [-0.10716596],\n",
       "       [ 0.25502738],\n",
       "       [ 0.18920395],\n",
       "       [ 0.6687284 ],\n",
       "       [ 0.11093731],\n",
       "       [ 0.30327293],\n",
       "       [ 0.95405126],\n",
       "       [ 0.40750444],\n",
       "       [-0.49362886],\n",
       "       [-1.0573905 ],\n",
       "       [ 0.68728936],\n",
       "       [ 0.18519959],\n",
       "       [-1.298243  ],\n",
       "       [ 0.24080732],\n",
       "       [-1.1281989 ],\n",
       "       [ 0.44090584],\n",
       "       [-1.049028  ],\n",
       "       [-0.34580863],\n",
       "       [ 0.53724146],\n",
       "       [-0.6672616 ],\n",
       "       [ 0.7510875 ],\n",
       "       [ 0.5921714 ],\n",
       "       [ 1.0459387 ],\n",
       "       [ 0.8822328 ],\n",
       "       [-0.5058749 ],\n",
       "       [-0.04526654],\n",
       "       [ 0.6563108 ],\n",
       "       [-0.596297  ],\n",
       "       [ 0.13957018],\n",
       "       [ 0.87399966],\n",
       "       [ 0.68260646],\n",
       "       [-0.16493785],\n",
       "       [ 0.58849406],\n",
       "       [-1.0421638 ],\n",
       "       [ 0.9329643 ],\n",
       "       [-0.39702046],\n",
       "       [ 0.55989707],\n",
       "       [ 0.15608345],\n",
       "       [ 0.14372538],\n",
       "       [ 0.73238844],\n",
       "       [-0.5415104 ],\n",
       "       [ 0.6633782 ],\n",
       "       [ 0.58486104],\n",
       "       [-1.137548  ],\n",
       "       [ 0.14953522],\n",
       "       [ 0.26656422],\n",
       "       [ 0.47476143],\n",
       "       [-0.8436967 ],\n",
       "       [ 0.288971  ],\n",
       "       [-0.8371541 ],\n",
       "       [-0.48666543],\n",
       "       [ 0.20708957],\n",
       "       [ 0.2712705 ],\n",
       "       [ 0.4715214 ],\n",
       "       [ 0.2652171 ],\n",
       "       [ 0.94158965],\n",
       "       [-0.14155078],\n",
       "       [ 0.59386665],\n",
       "       [-0.69510204],\n",
       "       [ 0.5236256 ],\n",
       "       [ 0.21673103],\n",
       "       [ 0.65530044],\n",
       "       [-0.8171511 ],\n",
       "       [ 0.5304326 ],\n",
       "       [ 0.9213896 ],\n",
       "       [ 0.65088403],\n",
       "       [-0.34184954],\n",
       "       [ 0.65561616],\n",
       "       [ 0.4195907 ],\n",
       "       [ 0.86856097],\n",
       "       [-0.49070758],\n",
       "       [ 0.9030379 ],\n",
       "       [-0.40997162],\n",
       "       [ 0.65172136],\n",
       "       [ 0.9541237 ],\n",
       "       [ 0.9751615 ],\n",
       "       [ 0.00672752],\n",
       "       [ 0.7240257 ],\n",
       "       [ 0.906465  ],\n",
       "       [-1.1239803 ],\n",
       "       [ 0.91424775],\n",
       "       [-1.1922162 ],\n",
       "       [-1.2282977 ],\n",
       "       [-0.49562716],\n",
       "       [ 0.1544856 ],\n",
       "       [ 0.8762835 ],\n",
       "       [-0.95616764],\n",
       "       [-0.8275098 ],\n",
       "       [-0.44796413],\n",
       "       [ 0.81976336],\n",
       "       [ 0.28451502],\n",
       "       [ 0.3232573 ],\n",
       "       [ 0.40234834],\n",
       "       [ 0.9187785 ],\n",
       "       [-1.1049494 ],\n",
       "       [ 0.24544959],\n",
       "       [-0.98709625],\n",
       "       [ 0.37906182],\n",
       "       [-0.77218264],\n",
       "       [ 0.8319861 ],\n",
       "       [ 0.74799633],\n",
       "       [ 0.42776033]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = pickle.load(open('fitted-model.pkl','rb'))\n",
    "pipeline.predict(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test saving and loading and using a pipeline (multi-threaded conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pickle.load(open('fitted-model.pkl','rb'))\n",
    "\n",
    "import os\n",
    "\n",
    "def init():\n",
    "    global pipeline\n",
    "    os.environ['CUDA_VISIBLE_DEVICES']=''\n",
    "    pipeline2 = pickle.load(open('fitted-model.pkl','rb'))\n",
    "\n",
    "with mp.Pool(4,initializer=init) as pool:\n",
    "    predictions = list(tqdm.tqdm(pool.imap(lambda x: pipeline2.predict(x),\n",
    "                                           docs,\n",
    "                                           chunksize=40),total=len(docs)))\n",
    "\n",
    "    \n",
    "# SDT = pipeline.named_steps.structuredatatransformer\n",
    "\n",
    "#     #pipeline.predict(docs)\n",
    "\n",
    "\n",
    "\n",
    "# SDT_out = SDT.transform(docs)\n",
    "\n",
    "# with mp.Pool(4) as pool:\n",
    "#     SDT_list = ListDataset(list(tqdm.tqdm(pool.imap(lambda x: SDT_out[x],range(len(SDT_out)),\n",
    "#                                                     chunksize=40,\n",
    "#                                                    initializer=init),total=len(SDT_out))))\n",
    "\n",
    "# pipeline.named_steps.neuralnetregressor.predict([SDT_list[0:1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.named_steps.neuralnetregressor.predict([SDT_list[0:1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDT_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.named_steps.neuralnetregressor.predict(SDT_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of using the pipeline to do conversion and everything (works, but is slow because of datset loading issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# #y, energies\n",
    "# target_list = scaler.transform(np.array([doc['energy'] for doc in docs]).reshape(-1,1))\n",
    "# y = torch.FloatTensor(target_list)\n",
    "\n",
    "# #Make the sklearn pipeline (convert doc, then net)\n",
    "# pipe = make_pipeline(SDT, net)\n",
    "\n",
    "# #Fit the pipeline\n",
    "# pipe.fit(docs,y=y)\n",
    "\n",
    "# #Load the best parameters (best validation)\n",
    "# net.load_params('params.pt')\n",
    "\n",
    "# #Save the fitted sklearn-compatible pipeline\n",
    "# with open('fitted-pipeline.pkl','wb') as fhandle:\n",
    "#     pickle.dump(pipe,fhandle)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
